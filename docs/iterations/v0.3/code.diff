diff --git a/main.py b/main.py
index f508fdd..d721bad 100644
--- a/main.py
+++ b/main.py
@@ -1,235 +1,399 @@
+import os
+import json
 import requests
 import psycopg2
-import time
-from datetime import datetime
+from datetime import datetime, timezone
 from tabulate import tabulate
 import smtplib
 from email.mime.text import MIMEText
 from email.mime.multipart import MIMEMultipart
 import pandas as pd
 from dotenv import load_dotenv
+
 load_dotenv()
-import os
 
-DB_CONFIG = {
-    "host": os.getenv("DB_HOST"),
+# ====== clts_pcp (monitoriza├º├úo) ======
+try:
+    import clts_pcp as clts  # pip install clts_pcp
+    HAS_CLTS = True
+except Exception:
+    HAS_CLTS = False
+
+
+# ====== Defaults  ======
+DEFAULT_EMAIL_FROM = "estagio.pipeline@example.com"
+DEFAULT_EMAIL_TO = ["pedro.pimenta@cm-maia.pt", "gustavo.sa.martins@gmail.com"]
+DEFAULT_PIPELINE_NAME = "GM-METEO"
+
+DEFAULT_DB_TARGETS_FILE = "db_targets.json"
+
+# Fallback DB (se n├úo houver targets file)
+DEFAULT_DB_CONFIG = {
+    "host": os.getenv("DB_HOST", ""),
     "port": int(os.getenv("DB_PORT", "5432")),
-    "dbname": os.getenv("DB_NAME"),
-    "user": os.getenv("DB_USER"),
-    "password": os.getenv("DB_PASSWORD"),
+    "dbname": os.getenv("DB_NAME", ""),
+    "user": os.getenv("DB_USER", ""),
+    "password": os.getenv("DB_PASSWORD", ""),
     "sslmode": os.getenv("DB_SSLMODE", "require"),
 }
 
-WEATHERBIT_KEY = os.getenv("WEATHERBIT_API_KEY")
+# Weatherbit (se tiver key)
+WEATHERBIT_KEY = os.getenv("WEATHERBIT_API_KEY", "")
 WEATHERBIT_CITY = os.getenv("WEATHERBIT_CITY", "Maia,PT")
-API_URL = f"https://api.weatherbit.io/v2.0/current?city={WEATHERBIT_CITY}&key={WEATHERBIT_KEY}"
+WEATHERBIT_URL = f"https://api.weatherbit.io/v2.0/current?city={WEATHERBIT_CITY}&key={WEATHERBIT_KEY}"
 
+# IPMA (se quiser usar)
+IPMA_URL = "https://api.ipma.pt/open-data/observation/meteorology/stations/observations.json"
 
-DEFAULT_EMAIL_FROM = "estagio.pipeline@example.com"
-DEFAULT_EMAIL_TO = ["pedro.pimenta@cm-maia.pt", "gustavo.sa.martins@gmail.com"]
-
-DEFAULT_PIPELINE_NAME = "GM-IPMA-METEO"
-
+# Default API URL (fallback)
+DEFAULT_API_URL = WEATHERBIT_URL if WEATHERBIT_KEY else IPMA_URL
 
-# ====== CONTEXTO (context aware) ======
 
+#  SQL (inser├º├úo + checker)
+INSERT_SQL = """
+    INSERT INTO meteo (
+        fonte, data, temp, humidade, vento, pressao, precipitacao,
+        lugar, lat, lon, regdata
+    )
+    VALUES (
+        %(fonte)s,
+        %(data)s,
+        %(temp)s,
+        %(humidade)s,
+        %(vento)s,
+        %(pressao)s,
+        %(precipitacao)s,
+        %(lugar)s,
+        %(lat)s,
+        %(lon)s,
+        NOW()
+    )
+"""
+
+# Checker por timestamp (recomendado para evitar duplica├º├úo por rerun)
+CHECK_DUPLICATE_SQL = """
+    SELECT 1
+    FROM meteo
+    WHERE fonte = %(fonte)s
+      AND data = %(data)s
+      AND (lugar IS NOT DISTINCT FROM %(lugar)s)
+    LIMIT 1
+"""
+
+# Se quiseres ΓÇ£por diaΓÇ¥, troca por este e ajusta o dict de params se necess├írio:
+# CHECK_DUPLICATE_SQL = """
+#     SELECT 1
+#     FROM meteo
+#     WHERE fonte = %(fonte)s
+#       AND CAST(data AS DATE) = CAST(%(data)s AS DATE)
+#       AND (lugar IS NOT DISTINCT FROM %(lugar)s)
+#     LIMIT 1
+# """
+
+
+# ====== CONTEXTO ======
 def get_context():
     user = os.getenv("PIPELINE_USER", "gustavo")
     env = os.getenv("PIPELINE_ENV", "local")
 
-    db_config = {
-        "dbname": os.getenv("DB_MAIN_NAME", DEFAULT_DB_CONFIG["dbname"]),
-        "user": os.getenv("DB_MAIN_USER", DEFAULT_DB_CONFIG["user"]),
-        "password": os.getenv("DB_MAIN_PASSWORD", DEFAULT_DB_CONFIG["password"]),
-        "host": os.getenv("DB_MAIN_HOST", DEFAULT_DB_CONFIG["host"]),
-        "port": int(os.getenv("DB_MAIN_PORT", DEFAULT_DB_CONFIG["port"])),
-        "sslmode": os.getenv("DB_MAIN_SSLMODE", DEFAULT_DB_CONFIG["sslmode"]),
-    }
-
-    # Nome da pipeline
     pipeline_name = os.getenv("PIPELINE_NAME", DEFAULT_PIPELINE_NAME)
 
-    # API URL para IPMA
-    api_url = os.getenv("PIPELINE_API_URL_IPMA", DEFAULT_API_URL)
+    # Mant├⌐m compatibilidade: aceita PIPELINE_API_URL (novo) e PIPELINE_API_URL_IPMA (antigo)
+    api_url = os.getenv("PIPELINE_API_URL", os.getenv("PIPELINE_API_URL_IPMA", DEFAULT_API_URL))
 
-    # Emails
     email_from = os.getenv("PIPELINE_EMAIL_FROM", DEFAULT_EMAIL_FROM)
-    email_to_raw = os.getenv(
-        "PIPELINE_EMAIL_TO",
-        ",".join(DEFAULT_EMAIL_TO)
-    )
+    email_to_raw = os.getenv("PIPELINE_EMAIL_TO", ",".join(DEFAULT_EMAIL_TO))
     email_to = [e.strip() for e in email_to_raw.split(",") if e.strip()]
 
-    # Endpoint / dashboard onde se podem ver os dados (para p├┤r no email)
     dashboard_url = os.getenv("PIPELINE_DASHBOARD_URL_METEO", "")
 
+    db_targets_file = os.getenv("PIPELINE_DB_TARGETS_FILE", DEFAULT_DB_TARGETS_FILE)
+
     return {
         "user": user,
         "env": env,
-        "db_config": db_config,
         "pipeline_name": pipeline_name,
         "api_url": api_url,
         "email_from": email_from,
         "email_to": email_to,
         "dashboard_url": dashboard_url,
+        "db_targets_file": db_targets_file,
     }
 
 
-# ====== 1. REQUEST / RECEIVE / PARSE ======
-
+# ====== helpers (tempo / monitor) ======
+def _clts_seconds(dt_obj):
+    """
+    clts.deltat() guarda watch/proc (no README mostra 2 colunas).
+    Isto tenta apanhar "watch time" em segundos de forma tolerante.
+    """
+    if isinstance(dt_obj, (tuple, list)) and len(dt_obj) >= 1:
+        return float(dt_obj[0])
+    if isinstance(dt_obj, dict):
+        for k in ("watch", "watch_time", "elapsed", "secs", "seconds"):
+            if k in dt_obj:
+                return float(dt_obj[k])
+    try:
+        return float(dt_obj)
+    except Exception:
+        return 0.0
+
+
+class StepMonitor:
+    def __init__(self, ctx):
+        self.ctx = ctx
+        self.summary = []
+
+        self.use_clts = HAS_CLTS
+        if self.use_clts:
+            clts.setcontext(f"{ctx['pipeline_name']} ({ctx['env']})")
+            self._ts_prev = clts.getts()
+
+    def mark(self, step_name, status="OK"):
+        if self.use_clts:
+            dt = clts.deltat(self._ts_prev)
+            clts.elapt[f"{step_name} ({status})."] = dt
+            secs = round(_clts_seconds(dt), 2)
+            self._ts_prev = clts.getts()
+        else:
+            # fallback simples se n├úo tiver o pacote instalado
+            secs = 0.0
+
+        self.summary.append([step_name, status, secs])
+        return secs
+
+    def html_table(self):
+        if self.use_clts:
+            # README: listtimes() devolve HTML quando usado como retorno
+            return clts.listtimes()
+        return tabulate(self.summary, headers=["Etapa", "Status", "Watch Time (s)"], tablefmt="html")
+
+
+# ====== 1) REQUEST / RECEIVE ======
 def request_data(api_url):
-    start = time.time()
-    response = requests.get(api_url, timeout=10)
-    response.raise_for_status()
-    elapsed = round(time.time() - start, 2)
-    return response.json(), elapsed
+    resp = requests.get(api_url, timeout=15)
+    resp.raise_for_status()
+    return resp.json()
 
 
 def receive_data(json_data):
-    start = time.time()
     if not json_data:
         raise ValueError("JSON vazio ou inv├ílido")
-    elapsed = round(time.time() - start, 2)
-    return json_data, elapsed
+    return json_data
 
 
-def parse_data(json_data):
-    start = time.time()
-    parsed = []
+# ====== 2) PARSE (compat├¡vel Weatherbit OU IPMA) ======
+def _parse_ts_any(value):
+    if isinstance(value, datetime):
+        return value
+    if isinstance(value, (int, float)):
+        # epoch
+        return datetime.fromtimestamp(value, tz=timezone.utc)
+
+    s = str(value).strip()
+    if s.endswith("Z"):
+        s = s[:-1] + "+00:00"
 
-    for timestamp, stations in json_data.items():
-        if not isinstance(stations, dict):
+    # tenta ISO
+    try:
+        dt = datetime.fromisoformat(s)
+        return dt if dt.tzinfo else dt.replace(tzinfo=timezone.utc)
+    except Exception:
+        pass
+
+    # tenta formatos comuns
+    for fmt in ("%Y-%m-%d %H:%M", "%Y-%m-%d %H:%M:%S", "%Y-%m-%dT%H:%M:%S"):
+        try:
+            dt = datetime.strptime(s, fmt)
+            return dt.replace(tzinfo=timezone.utc)
+        except Exception:
             continue
 
-        for station_id, values in stations.items():
-            if not isinstance(values, dict):
+    # ├║ltimo fallback: agora
+    return datetime.now(timezone.utc)
+
+
+def parse_data(json_data):
+    # Weatherbit
+    if isinstance(json_data, dict) and "data" in json_data and isinstance(json_data["data"], list) and json_data["data"]:
+        obs = json_data["data"][0]
+
+        # weatherbit costuma trazer ob_time 
+        if "ob_time" in obs:
+            dt = _parse_ts_any(obs["ob_time"])
+        elif "ts" in obs:
+            dt = _parse_ts_any(int(obs["ts"]))
+        else:
+            dt = datetime.now(timezone.utc)
+
+        return [{
+            "fonte": "WEATHERBIT",
+            "data": dt,
+            "temp": obs.get("temp"),
+            "humidade": obs.get("rh"),
+            "vento": obs.get("wind_spd"),
+            "pressao": obs.get("pres"),
+            "precipitacao": obs.get("precip"),
+            "lugar": obs.get("city_name") or WEATHERBIT_CITY,
+            "lat": obs.get("lat"),
+            "lon": obs.get("lon"),
+        }]
+
+    # IPMA
+    parsed = []
+    if isinstance(json_data, dict):
+        for timestamp, stations in json_data.items():
+            if not isinstance(stations, dict):
                 continue
 
-            parsed.append({
-                "fonte": "IPMA",
-                "data": timestamp,
-                "temp": values.get("temperatura"),
-                "humidade": values.get("humidade"),
-                "vento": values.get("intensidadeVento"),
-                "vento_km": values.get("intensidadeVentoKM"),
-                "pressao": values.get("pressao"),
-                "precipitacao": values.get("precAcumulada"),
-                "radiacao": values.get("radiacao"),
-                "id_estacao": station_id,
-                "id_direcc_vento": values.get("idDireccVento"),
-                "lugar": None,
-                "lat": None,
-                "lon": None,
+            ts_dt = _parse_ts_any(timestamp)
+
+            for station_id, values in stations.items():
+                if not isinstance(values, dict):
+                    continue
+
+                parsed.append({
+                    "fonte": "IPMA",
+                    "data": ts_dt,
+                    "temp": values.get("temperatura"),
+                    "humidade": values.get("humidade"),
+                    "vento": values.get("intensidadeVento"),
+                    "pressao": values.get("pressao"),
+                    "precipitacao": values.get("precAcumulada"),
+                    # aqui uso lugar=station_id para conseguir deduplicar por esta├º├úo
+                    "lugar": str(station_id),
+                    "lat": None,
+                    "lon": None,
+                })
+
+    return parsed
+
+
+# ====== 3) DB targets (ficheiro lista) ======
+def load_db_targets(ctx):
+    path = ctx["db_targets_file"]
+
+    if path and os.path.exists(path):
+        with open(path, "r", encoding="utf-8") as f:
+            raw = json.load(f)
+
+        targets = raw["targets"] if isinstance(raw, dict) and "targets" in raw else raw
+        if not isinstance(targets, list):
+            raise ValueError("db_targets.json inv├ílido: esperado uma lista ou {'targets':[...]}")
+
+        # normaliza
+        norm = []
+        for i, t in enumerate(targets, start=1):
+            if not isinstance(t, dict):
+                continue
+            name = t.get("name") or f"db{i}"
+            norm.append({
+                "name": name,
+                "host": t.get("host", ""),
+                "port": int(t.get("port", 5432)),
+                "dbname": t.get("dbname", ""),
+                "user": t.get("user", ""),
+                "password": t.get("password", ""),
+                "sslmode": t.get("sslmode", "require"),
             })
+        return norm
+
+    # fallback: 1 target via env DB_HOST/DB_PORT/...
+    fb = DEFAULT_DB_CONFIG.copy()
+    fb["name"] = "default_env"
+    return [fb]
+
+
+def connect_all_dbs(targets):
+    conns = {}
+    errors = {}
 
-    elapsed = round(time.time() - start, 2)
-    return parsed, elapsed
+    for t in targets:
+        name = t.get("name", "db")
+        host = (t.get("host") or "").strip()
 
+        # mant├⌐m o teu ΓÇ£modo offlineΓÇ¥ se host vazio ou "Nao..."
+        if not host or host.lower().startswith("nao"):
+            errors[name] = "offline/no-host"
+            continue
 
-# ====== 2. LIGA├ç├âO BD (ainda com modo offline) ======
+        try:
+            conn = psycopg2.connect(
+                host=t["host"],
+                port=int(t["port"]),
+                dbname=t["dbname"],
+                user=t["user"],
+                password=t["password"],
+                sslmode=t.get("sslmode", "require"),
+            )
+            conns[name] = conn
+        except Exception as e:
+            errors[name] = str(e)
+
+    return conns, errors
+
+
+def close_all(conns):
+    for _, c in conns.items():
+        try:
+            c.close()
+        except Exception:
+            pass
 
-def connect_db(ctx):
-    start = time.time()
-    db_conf = ctx["db_config"]
 
-    # Se o host ainda ├⌐ "Nao tenho acesso", assumimos modo offline
-    if not db_conf.get("host") or db_conf["host"].lower().startswith("nao"):
-        print("Liga├º├úo offline simulada ΓÇö base de dados n├úo utilizada.")
-        conn = None
-    else:
-        conn = psycopg2.connect(**db_conf)
-        print("Liga├º├úo ├á base de dados estabelecida.")
+# ====== 4) checker + insert ======
+def is_duplicate(conn, row):
+    params = {
+        "fonte": row["fonte"],
+        "data": row["data"],
+        "lugar": row.get("lugar"),
+    }
+    with conn.cursor() as cur:
+        cur.execute(CHECK_DUPLICATE_SQL, params)
+        return cur.fetchone() is not None
 
-    elapsed = round(time.time() - start, 2)
-    return conn, elapsed
 
+def insert_rows(conn, rows):
+    with conn.cursor() as cur:
+        cur.executemany(INSERT_SQL, rows)
+    conn.commit()
 
-def prepare_query(parsed_data):
-    start = time.time()
-    query = """
-        INSERT INTO meteo (
-            fonte, data, temp, humidade, vento, pressao, precipitacao,
-            lugar, lat, lon, regdata
-        )
-        VALUES (
-            %(fonte)s,
-            %(data)s,
-            %(temp)s,
-            %(humidade)s,
-            %(vento)s,
-            %(pressao)s,
-            %(precipitacao)s,
-            %(lugar)s,
-            %(lat)s,
-            %(lon)s,
-            NOW()
-        )
-    """
-    elapsed = round(time.time() - start, 2)
-    return query, elapsed
 
+def offline_dump(rows):
+    df = pd.DataFrame(rows)
+    os.makedirs("offline_output", exist_ok=True)
+    file_path = f"offline_output/meteo_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
+    df.to_csv(file_path, index=False)
+    return file_path
 
-def execute_query(conn, query, parsed_data, ctx):
-    """
-    Mant├⌐m o comportamento original:
-      - se conn for None ΓåÆ guarda em CSV offline
-      - se conn n├úo for None ΓåÆ (podemos mais tarde passar a inserir na BD)
-    """
-    start = time.time()
-
-    if conn is None:
-        # Modo offline 
-        df = pd.DataFrame(parsed_data)
-        os.makedirs("offline_output", exist_ok=True)
-        file_path = f"offline_output/meteo_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
-        df.to_csv(file_path, index=False)
-        print(f"Dados guardados offline em: {file_path}")
-    else:
-        
-        with conn.cursor() as cur:
-            cur.executemany(query, parsed_data)
-        conn.commit()
-        print(f"Dados inseridos na BD (pipeline {ctx['pipeline_name']}).")
-
-    elapsed = round(time.time() - start, 2)
-    return elapsed
-
-
-def close_connection(conn):
-    start = time.time()
-    if conn is not None:
-        conn.close()
-        print("Liga├º├úo ├á base de dados fechada.")
-    else:
-        print("Fecho de liga├º├úo offline ΓÇö nada a fechar.")
-    elapsed = round(time.time() - start, 2)
-    return elapsed
-
-
-# ====== 3. EMAIL ======
-
-def send_summary_email(summary_rows, ctx):
-    html_table = tabulate(summary_rows, headers=["Etapa", "Status", "Watch Time (s)"], tablefmt="html")
 
+# ====== 5) Email ======
+def send_summary_email(ctx, monitor, extra_lines=None):
     msg = MIMEMultipart("alternative")
     msg["Subject"] = f"Pipeline {ctx['pipeline_name']} - Relat├│rio de Execu├º├úo ({ctx['env']})"
     msg["From"] = ctx["email_from"]
     msg["To"] = ", ".join(ctx["email_to"])
 
-    extra_info = f"<p>Pipeline: <b>{ctx['pipeline_name']}</b> | Ambiente: <b>{ctx['env']}</b> | Utilizador l├│gico: <b>{ctx['user']}</b></p>"
+    extra_info = (
+        f"<p>Pipeline: <b>{ctx['pipeline_name']}</b> | Ambiente: <b>{ctx['env']}</b> "
+        f"| Utilizador l├│gico: <b>{ctx['user']}</b></p>"
+    )
 
-    # endpoint / dashboard, se existir
     if ctx["dashboard_url"]:
-        extra_info += f"""
-        <p>Pode verificar os dados em:
-           <a href="{ctx['dashboard_url']}">{ctx['dashboard_url']}</a>
-        </p>
-        """
+        extra_info += (
+            f'<p>Pode verificar os dados em: '
+            f'<a href="{ctx["dashboard_url"]}">{ctx["dashboard_url"]}</a></p>'
+        )
+
+    if extra_lines:
+        extra_info += "<br>" + "<br>".join(f"<p>{line}</p>" for line in extra_lines)
+
+    html_table = monitor.html_table()
 
     body = f"""
     <html>
     <body>
-        <p><b>Resumo da execu├º├úo da pipeline de meteorologia:</b></p>
+        <p><b>Resumo da execu├º├úo da pipeline:</b></p>
         {extra_info}
         {html_table}
         <br>
@@ -237,6 +401,7 @@ def send_summary_email(summary_rows, ctx):
     </body>
     </html>
     """
+
     msg.attach(MIMEText(body, "html"))
 
     try:
@@ -247,93 +412,80 @@ def send_summary_email(summary_rows, ctx):
         print(f"Falha ao enviar email: {e}")
 
 
-# ====== 4. PIPELINES ======
+# ====== PIPELINE (normal) ======
+def pipeline_meteo(ctx):
+    mon = StepMonitor(ctx)
+    extra_lines = []
 
-def pipeline_meteo_normal(ctx):
-    summary = []
-    conn = None
+    conns = {}
+    conn_errors = {}
 
     try:
         # Request
-        data, t1 = request_data(ctx["api_url"])
-        summary.append(["Request API", "OK", t1])
-
-        # Recep├º├úo
-        received, t2 = receive_data(data)
-        summary.append(["Recep├º├úo", "OK", t2])
-
-        # Parsing
-        parsed, t3 = parse_data(received)
-        summary.append(["Parsing", "OK", t3])
-
-        # Liga├º├úo BD
-        conn, t4 = connect_db(ctx)
-        summary.append(["Liga├º├úo BD", "OK", t4])
-
-        # Prepara├º├úo Query
-        query, t5 = prepare_query(parsed)
-        summary.append(["Prepara├º├úo Query", "OK", t5])
-
-        # Execu├º├úo Query (offline CSV ou BD, consoante o contexto)
-        t6 = execute_query(conn, query, parsed, ctx)
-        summary.append(["Execu├º├úo Query", "OK", t6])
-
-        # Fecho BD
-        t7 = close_connection(conn)
-        summary.append(["Fecho BD", "OK", t7])
+        data = request_data(ctx["api_url"])
+        mon.mark("Request API", "OK")
+
+        # Receive
+        received = receive_data(data)
+        mon.mark("Recep├º├úo", "OK")
+
+        # Parse
+        parsed = parse_data(received)
+        if not parsed:
+            raise ValueError("Parsing devolveu 0 linhas (verifica API/parse).")
+        mon.mark(f"Parsing ({len(parsed)} linhas)", "OK")
+
+        # Load targets
+        targets = load_db_targets(ctx)
+        mon.mark(f"Load DB targets ({len(targets)})", "OK")
+
+        # Connect all
+        conns, conn_errors = connect_all_dbs(targets)
+        extra_lines.append(f"DBs ligadas: {', '.join(conns.keys()) if conns else '(nenhuma)'}")
+        if conn_errors:
+            extra_lines.append("DBs com erro/offline: " + "; ".join([f"{k}={v}" for k, v in conn_errors.items()]))
+
+        mon.mark("Liga├º├úo BD(s)", "OK" if conns else "WARN")
+
+        # Se n├úo h├í conns ΓåÆ offline dump
+        if not conns:
+            fp = offline_dump(parsed)
+            mon.mark("Persist├¬ncia offline (CSV)", "OK")
+            extra_lines.append(f"CSV offline: {fp}")
+            return mon, extra_lines
+
+        # Insert por DB com checker
+        for dbname, conn in conns.items():
+            inserted = 0
+            skipped = 0
+
+            for row in parsed:
+                if is_duplicate(conn, row):
+                    skipped += 1
+                else:
+                    insert_rows(conn, [row])
+                    inserted += 1
+
+            mon.mark(f"DB insert [{dbname}] (ins={inserted}, skip={skipped})", "OK")
 
     except Exception as e:
-        summary.append(["Erro", f"FAIL ({e})", 0])
-    finally:
-        send_summary_email(summary, ctx)
-        print(tabulate(summary, headers=["Etapa", "Status", "Watch Time (s)"]))
-
-
-def pipeline_meteo_heavy(ctx):
-    summary = []
-    conn = None
-
-    try:
-        # Liga├º├úo BD primeiro
-        conn, t1 = connect_db(ctx)
-        summary.append(["Liga├º├úo BD", "OK", t1])
-
-        # Request
-        data, t2 = request_data(ctx["api_url"])
-        summary.append(["Request API", "OK", t2])
+        mon.mark("Erro", f"FAIL ({e})")
+        extra_lines.append(f"Erro: {e}")
 
-        # Recep├º├úo
-        received, t3 = receive_data(data)
-        summary.append(["Recep├º├úo", "OK", t3])
-
-        # Parsing
-        parsed, t4 = parse_data(received)
-        summary.append(["Parsing", "OK", t4])
-
-        # Prepara├º├úo Query
-        query, t5 = prepare_query(parsed)
-        summary.append(["Prepara├º├úo Query", "OK", t5])
-
-        # Execu├º├úo Query
-        t6 = execute_query(conn, query, parsed, ctx)
-        summary.append(["Execu├º├úo Query", "OK", t6])
-
-    except Exception as e:
+    finally:
         try:
-            if conn:
-                close_connection(conn)
+            close_all(conns)
+            mon.mark("Fecho BD(s)", "OK")
         except Exception:
             pass
-        summary.append(["Erro", f"FAIL ({e})", 0])
-    finally:
-        if conn:
-            t7 = close_connection(conn)
-            summary.append(["Fecho BD", "OK", t7])
-        send_summary_email(summary, ctx)
-        print(tabulate(summary, headers=["Etapa", "Status", "Watch Time (s)"]))
+
+        send_summary_email(ctx, mon, extra_lines=extra_lines)
+        print(tabulate(mon.summary, headers=["Etapa", "Status", "Watch Time (s)"]))
+
+    return mon, extra_lines
 
 
 if __name__ == "__main__":
     ctx = get_context()
-    print(f"Iniciando pipeline meteorol├│gica '{ctx['pipeline_name']}' (env={ctx['env']}, user={ctx['user']})...")
-    pipeline_meteo_normal(ctx)
+    print(f"Iniciando pipeline '{ctx['pipeline_name']}' (env={ctx['env']}, user={ctx['user']})...")
+    pipeline_meteo(ctx)
